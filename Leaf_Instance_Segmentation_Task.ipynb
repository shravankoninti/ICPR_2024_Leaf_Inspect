{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a414dafc-bd1e-499e-b0b0-6d7d1b26d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SEEDING DONE\n",
      "> SEEDING DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:09<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.3380, Val Loss: 1.0703, Mean SBD: 0.6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 0.8477, Val Loss: 0.7085, Mean SBD: 0.7689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 0.4323, Val Loss: 0.3114, Mean SBD: 0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 0.2280, Val Loss: 0.2036, Mean SBD: 0.8414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 0.1862, Val Loss: 0.1847, Mean SBD: 0.8483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 0.1695, Val Loss: 0.1686, Mean SBD: 0.8576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 0.1571, Val Loss: 0.1635, Mean SBD: 0.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 0.1518, Val Loss: 0.1796, Mean SBD: 0.8365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Train Loss: 0.1431, Val Loss: 0.1620, Mean SBD: 0.8528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Train Loss: 0.1385, Val Loss: 0.1547, Mean SBD: 0.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Train Loss: 0.1318, Val Loss: 0.1498, Mean SBD: 0.8657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Train Loss: 0.1308, Val Loss: 0.1565, Mean SBD: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Train Loss: 0.1288, Val Loss: 0.1515, Mean SBD: 0.8631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Train Loss: 0.1229, Val Loss: 0.1556, Mean SBD: 0.8564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Train Loss: 0.1177, Val Loss: 0.1507, Mean SBD: 0.8634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Train Loss: 0.1228, Val Loss: 0.1569, Mean SBD: 0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Train Loss: 0.1137, Val Loss: 0.1439, Mean SBD: 0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Train Loss: 0.1093, Val Loss: 0.1535, Mean SBD: 0.8597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Train Loss: 0.1050, Val Loss: 0.1457, Mean SBD: 0.8676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Train Loss: 0.1002, Val Loss: 0.1467, Mean SBD: 0.8665\n",
      "Best Validation Loss: 0.14389776363968848, Best Mean SBD: 0.8690464049577713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:02<00:00, 28.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted results saved to ./predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import Compose, Normalize, Resize, RandomCrop, HorizontalFlip, VerticalFlip, RandomBrightnessContrast\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    batch_size = 4\n",
    "    lr = 1e-3\n",
    "    num_epochs = 20\n",
    "    input_size = (512, 512)\n",
    "    model_name = \"timm-efficientnet-b4\"\n",
    "    train_image_path = './rgb'\n",
    "    train_mask_path = './label'\n",
    "    test_image_path = '../Test_data'\n",
    "    output_path = './predictions'\n",
    "    best_model_path = './best_model.pth'\n",
    "    log_dir = './logs'\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print('> SEEDING DONE')\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "# Custom dataset class for loading images and masks\n",
    "class LeafDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_files[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        mask = (mask > 0).float()  # Convert mask to binary (foreground vs. background)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "# Define transformations\n",
    "transform = Compose([\n",
    "    Resize(CFG.input_size[0], CFG.input_size[1]),\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Get image and mask files\n",
    "image_files = sorted(glob(os.path.join(CFG.train_image_path, '*.png')))\n",
    "mask_files = sorted(glob(os.path.join(CFG.train_mask_path, '*.png')))\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_img_files, val_img_files, train_mask_files, val_mask_files = train_test_split(image_files, mask_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = LeafDataset(train_img_files, train_mask_files, transform=transform)\n",
    "val_dataset = LeafDataset(val_img_files, val_mask_files, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=CFG.model_name, \n",
    "    encoder_weights='imagenet', \n",
    "    in_channels=3, \n",
    "    classes=1, \n",
    "    activation=None\n",
    ")\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        dice = self.dice_loss(inputs, targets)\n",
    "        bce = self.bce_loss(inputs, targets)\n",
    "        return dice + bce\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "\n",
    "# Function to calculate Dice coefficient\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = torch.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Function to calculate Symmetric Best Dice (SBD)\n",
    "def symmetric_best_dice(y_true, y_pred):\n",
    "    return (dice_coefficient(y_true, y_pred) + dice_coefficient(y_pred, y_true)) / 2\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "    best_sbd = 0\n",
    "\n",
    "    os.makedirs(CFG.log_dir, exist_ok=True)\n",
    "    log_file_path = os.path.join(CFG.log_dir, f'{CFG.model_name}_training_log.csv')\n",
    "\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write('epoch,train_loss,val_loss,mean_sbd\\n')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images = images.to('cuda')\n",
    "            masks = masks.to('cuda').unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        sbd_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to('cuda')\n",
    "                masks = masks.to('cuda').unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "                preds = (preds > 0.5).astype(np.uint8)\n",
    "                masks = masks.cpu().numpy()\n",
    "\n",
    "                for true, pred in zip(masks, preds):\n",
    "                    sbd_scores.append(symmetric_best_dice(torch.tensor(true).float(), torch.tensor(pred).float()).item())\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        mean_sbd = np.mean(sbd_scores)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Mean SBD: {mean_sbd:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_sbd = mean_sbd\n",
    "            torch.save(model.state_dict(), CFG.best_model_path)\n",
    "\n",
    "        # Save logs\n",
    "        with open(log_file_path, 'a') as f:\n",
    "            f.write(f'{epoch+1},{train_loss},{val_loss},{mean_sbd}\\n')\n",
    "\n",
    "    print(f'Best Validation Loss: {best_val_loss}, Best Mean SBD: {best_sbd}')\n",
    "\n",
    "    # Save the best mean SBD score\n",
    "    with open(os.path.join(CFG.log_dir, f'{CFG.model_name}_best_mean_sbd.txt'), 'w') as f:\n",
    "        f.write(f'{best_sbd:.4f}\\n')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=CFG.num_epochs)\n",
    "\n",
    "# Load the best model for predictions\n",
    "model.load_state_dict(torch.load(CFG.best_model_path))\n",
    "\n",
    "# Function to predict and save results for the test set\n",
    "def predict_and_save_results(model, test_image_path, output_path):\n",
    "    test_image_files = sorted(glob(os.path.join(test_image_path, '*.png')))\n",
    "    transform = Compose([\n",
    "        Resize(CFG.input_size[0], CFG.input_size[1]),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for img_file in tqdm(test_image_files):\n",
    "        img = cv2.imread(img_file)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        augmented = transform(image=img_rgb)\n",
    "        img_transformed = augmented['image'].unsqueeze(0).to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_transformed)\n",
    "            preds = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (preds > 0.5).astype(np.uint8)\n",
    "            preds = cv2.resize(preds[0, 0], (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        output_file = os.path.join(output_path, os.path.basename(img_file).replace('.png', '_result.png'))\n",
    "        cv2.imwrite(output_file, preds * 255)\n",
    "\n",
    "# Predict and save results for the test set\n",
    "predict_and_save_results(model, CFG.test_image_path, CFG.output_path)\n",
    "\n",
    "print(f\"Predicted results saved to {CFG.output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d103ee9-d2fd-4ad1-ae5e-fd22967710df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xeek_env",
   "language": "python",
   "name": "xeek_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
